# -*- coding: utf-8 -*-
"""Project_Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fl3gm5feNRf5D0qDcCu-6K6oPrewurLn

## Import Library
"""

# Menginstal berbagai library Python yang dibutuhkan
!pip install tensorflow tensorflowjs keras opendatasets pandas numpy matplotlib Pillow scipy scikit-learn seaborn

# Commented out IPython magic to ensure Python compatibility.
# Library yang sering digunakan
import os
import zipfile
from zipfile import ZipFile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import uuid
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from tqdm import tqdm
from tqdm.notebook import tqdm as tq
from google.colab import files

# Libraries untuk pemrosesan data gambar
import cv2
from PIL import Image, ImageOps
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

# Libraries untuk pembangunan model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import mixed_precision

# menonaktifkan warning yang mungkin muncul, seperti warning FutureWarning.
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Load Dataset"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/

# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d mostafaabla/garbage-classification
!unzip garbage-classification.zip

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/

# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d sumn2u/garbage-classification-v2
!unzip garbage-classification-v2.zip

!wget https://github.com/fannyahdita/DSA_tugas_akhir/raw/master/dataset-sampah.zip -O dataset-sampah.zip
!unzip dataset-sampah.zip -d dataset_sampah

"""## Preprocessing"""

# Lokasi folder asal dan target
base_path = "/content/garbage_classification"
target_folder = os.path.join(base_path, "glass")
os.makedirs(target_folder, exist_ok=True)

# Folder-foler glass varian
glass_variants = ["brown-glass", "green-glass", "white-glass"]

# Gabungkan isi semua varian ke folder "glass"
for variant in glass_variants:
    variant_path = os.path.join(base_path, variant)
    for filename in os.listdir(variant_path):
        src = os.path.join(variant_path, filename)

        if not os.path.isfile(src):
            continue

        dst = os.path.join(target_folder, filename)

        # Hindari overwrite
        if os.path.exists(dst):
            base, ext = os.path.splitext(filename)
            i = 1
            while os.path.exists(dst):
                dst = os.path.join(target_folder, f"{base}_{i}{ext}")
                i += 1

        shutil.copy2(src, dst)

print("Penyatuan folder glass selesai.")

folders_to_delete = ["brown-glass", "green-glass", "white-glass"]
base_path = "/content/garbage_classification"

for folder in folders_to_delete:
    full_path = os.path.join(base_path, folder)
    if os.path.exists(full_path):
        shutil.rmtree(full_path)
        print(f"Folder '{folder}' dihapus.")
    else:
        print(f"Folder '{folder}' tidak ditemukan.")

!mv dataset_sampah/dataset-sampah/* dataset_sampah/
!rm -r dataset_sampah/dataset-sampah

# Path folder yang akan digabung
source_dirs = ["/content/garbage_classification", "/content/garbage-dataset", "/content/dataset_sampah"]
target_dir = "/content/combined_dataset"

# Buat folder gabungan
os.makedirs(target_dir, exist_ok=True)

# Gabungkan semua folder label
for src_dir in source_dirs:
    for label in os.listdir(src_dir):
        src_label_path = os.path.join(src_dir, label)

        if not os.path.isdir(src_label_path):
            continue  # Lewati file jika ada

        dst_label_path = os.path.join(target_dir, label)
        os.makedirs(dst_label_path, exist_ok=True)

        for filename in os.listdir(src_label_path):
            src_file = os.path.join(src_label_path, filename)

            if not os.path.isfile(src_file):
                continue

            dst_file = os.path.join(dst_label_path, filename)

            # Hindari overwrite jika nama file sama
            if os.path.exists(dst_file):
                base, ext = os.path.splitext(filename)
                i = 1
                while os.path.exists(dst_file):
                    dst_file = os.path.join(dst_label_path, f"{base}_{i}{ext}")
                    i += 1

            shutil.copy2(src_file, dst_file)

print("Penggabungan selesai. Folder gabungan ada di:", target_dir)

for label in os.listdir("/content/combined_dataset"):
    folder_path = os.path.join("/content/combined_dataset", label)
    if os.path.isdir(folder_path):
        count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
        print(f"{label}: {count} file")

wrong_label_path = os.path.join(target_dir, "dataset-sampah")

if os.path.exists(wrong_label_path):
    shutil.rmtree(wrong_label_path)
    print("Folder 'dataset-sampah' berhasil dihapus dari dataset gabungan.")
else:
    print("Folder 'dataset-sampah' tidak ditemukan di folder gabungan.")

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
lung_image = {}

# Tentukan path sumber train
path = "/content/combined_dataset"
for i in os.listdir(path):
    lung_image[i] = os.listdir(os.path.join(path, i))

# Menampilkan secara acak 5 gambar di bawah setiap dari 2 kelas dari data.
# Anda akan melihat gambar yang berbeda setiap kali kode ini dijalankan.
path = "/content/combined_dataset"

# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih
fig, axs = plt.subplots(len(lung_image.keys()), 5, figsize=(15, 15))

for i, class_name in enumerate(os.listdir(path)):
    images = np.random.choice(lung_image[class_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path, class_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])


fig.tight_layout()

import glob

base_dir = "/content/combined_dataset"
data = []

for label in os.listdir(base_dir):
    label_folder = os.path.join(base_dir, label)
    if not os.path.isdir(label_folder):
        continue
    for file in os.listdir(label_folder):
        if file.endswith((".jpg", ".jpeg", ".png")):
            path = os.path.join(label_folder, file)
            data.append({'path': path, 'file_name': file, 'labels': label})

distribution_train = pd.DataFrame(data)

"""## Augmentasi Dataset"""

# --- Fungsi augmentasi sederhana ---
def augment_and_save(src_path, save_dir):
    img = Image.open(src_path)

    # Pilih augmentasi secara acak
    op = random.choice(['flip', 'rotate'])
    if op == 'flip':
        aug = ImageOps.mirror(img)
    else:
        angle = random.choice([90, 180, 270])
        aug = img.rotate(angle, expand=True)

    # Konversi ke RGB jika bukan RGB
    if aug.mode != 'RGB':
        aug = aug.convert("RGB")

    # Simpan dengan nama unik
    new_name = f"{uuid.uuid4().hex}.png"
    save_path = os.path.join(save_dir, new_name)
    aug.save(save_path)

    return new_name, save_path

# --- Hitung target balancing ---
class_counts = distribution_train['labels'].value_counts()
min_count    = class_counts.min()
target = min(min_count * 5, 5000)

# Folder output
output_dir = "/content/balanced_dataset"
os.makedirs(output_dir, exist_ok=True)

# DataFrame baru untuk menyimpan path hasil oversampling+augmentasi
new_rows = []

for label, group in distribution_train.groupby('labels'):
    label_folder = os.path.join(output_dir, label)
    os.makedirs(label_folder, exist_ok=True)

    if len(group) > target:
        # Undersample: ambil random subset
        sampled = group.sample(n=target, random_state=42)
        for _, row in tqdm(sampled.iterrows(), total=len(sampled), desc=f"Copying {label}"):
            dst = os.path.join(label_folder, row['file_name'])
            shutil.copyfile(row['path'], dst)
            new_rows.append({'path': dst, 'file_name': row['file_name'], 'labels': label})
    else:
        # Oversample + augmentasi
        repeats = target // len(group)
        rem     = target % len(group)

        # Ulangi full group beberapa kali
        for _ in range(repeats):
            for _, row in tqdm(group.iterrows(), total=len(group), desc=f"Augmenting {label} (repeat)"):
                fn, saved = augment_and_save(row['path'], label_folder)
                new_rows.append({'path': saved, 'file_name': fn, 'labels': label})

        # Ambil sisanya secara acak dan augment
        extra = group.sample(n=rem, random_state=42)
        for _, row in tqdm(extra.iterrows(), total=len(extra), desc=f"Augmenting {label} (extra)"):
            fn, saved = augment_and_save(row['path'], label_folder)
            new_rows.append({'path': saved, 'file_name': fn, 'labels': label})

# Gabungkan semua metadata jadi DataFrame
balanced_df = pd.DataFrame(new_rows)

# Menghitung jumlah gambar per kelas setelah proses oversampling + undersampling
class_counts_after = balanced_df['labels'].value_counts()

# Membuat plot bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=class_counts_after.index, y=class_counts_after.values, palette='viridis')

# Menambahkan label dan judul
plt.title("Distribusi Jumlah Gambar per Kelas Setelah Augmentasi & Undersampling", fontsize=14)
plt.xlabel("Kelas", fontsize=12)
plt.ylabel("Jumlah Gambar", fontsize=12)
plt.xticks(rotation=45)  # Rotasi label kelas jika terlalu panjang

# Menampilkan grafik
plt.tight_layout()
plt.show()

# Dalam bentuk DataFrame yang lebih rapi
print("Jumlah gambar per kelas setelah augmentasi & undersampling:")
print(class_counts_after.reset_index().rename(columns={'index': 'Label', 'labels': 'Jumlah Gambar'}))

"""## Split Dataset"""

# Direktori dataset
dataset_path = '/content/balanced_dataset'
base_output = 'split_dataset'

# Membuat direktori output jika belum ada
for split in ['train', 'val', 'test']:
    os.makedirs(os.path.join(base_output, split), exist_ok=True)

# Mengumpulkan semua file gambar beserta labelnya
image_paths = []
labels = []

for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):
        for img in os.listdir(class_path):
            image_paths.append(os.path.join(class_path, img))
            labels.append(class_name)

# Membuat DataFrame
df = pd.DataFrame({'path': image_paths, 'labels': labels})

# Split dataset menjadi train (80%), val (10%), test (10%)
X_train, X_temp, y_train, y_temp = train_test_split(df['path'], df['labels'], test_size=0.2, random_state=300)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=300)

# Membuat DataFrame untuk setiap subset
df_train = pd.DataFrame({'path': X_train, 'labels': y_train, 'set': 'train'})
df_val = pd.DataFrame({'path': X_val, 'labels': y_val, 'set': 'val'})
df_test = pd.DataFrame({'path': X_test, 'labels': y_test, 'set': 'test'})

# Gabungkan semua DataFrame
df_all = pd.concat([df_train, df_val, df_test], ignore_index=True)

# Memindahkan gambar ke direktori yang sesuai
for index, row in df_all.iterrows():
    class_output_dir = os.path.join(base_output, row['set'], row['labels'])
    os.makedirs(class_output_dir, exist_ok=True)  # Pastikan direktori label ada
    shutil.copy(row['path'], os.path.join(class_output_dir, os.path.basename(row['path'])))

# Print informasi jumlah data
print('=====================================================')
print(df_all.groupby(['set', 'labels']).size())
print('=====================================================')

# Cek sample data
print(df_all.sample(5))

base_dir = 'split_dataset'
img_size = (150, 150)
batch_size = 32

# Membuat objek ImageDataGenerator untuk normalisasi gambar
datagen = ImageDataGenerator(rescale=1/255., validation_split=0.1)
test_datagen = ImageDataGenerator(rescale=1/255.)

# Training set
train_generator = datagen.flow_from_directory(
    os.path.join(base_dir, 'train'),
    batch_size=batch_size,
    target_size=img_size,
    color_mode="rgb",
    class_mode='categorical',
    shuffle=True
)

# Validation set
validation_generator = datagen.flow_from_directory(
    os.path.join(base_dir, 'val'),
    batch_size=batch_size,
    target_size=img_size,
    color_mode="rgb",
    class_mode='categorical',
    shuffle=False
)

# Test set
test_generator = test_datagen.flow_from_directory(
    os.path.join(base_dir, 'test'),
    batch_size=batch_size,
    target_size=img_size,
    color_mode="rgb",
    class_mode='categorical',
    shuffle=False
)

"""## Membangun dan Melatih Model"""

# Aktifkan mixed precision policy
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Load model pretrained MobileNetV2
pre_trained_model = MobileNetV2(
    weights="imagenet",
    include_top=False,
    input_shape=(150, 150, 3)
)

# Nonaktifkan training pada semua layer MobileNetV2
for layer in pre_trained_model.layers:
    layer.trainable = False

# Bangun model lengkap
model = Sequential([
    pre_trained_model,                    # Backbone dari pretrained model
    GlobalAveragePooling2D(),            # Pooling global untuk fitur kompak
    Dropout(0.3),

    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),

    # Kelas terakhir dengan dtype float32 agar aman di mixed precision
    Dense(train_generator.num_classes, activation='softmax', dtype='float32')
])

# Kompilasi model
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Tampilkan arsitektur
model.summary()

# Compile model dengan optimasi RAM
model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callback untuk menyimpan model terbaik
checkpoint = ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True, mode='min')

# Early stopping untuk menghentikan jika tidak ada peningkatan
early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True, mode='max')

# Learning rate decay
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

# Tampilkan arsitektur
model.summary()

# Melatih model
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10,
    callbacks=[checkpoint, early_stopping, reduce_lr],
    verbose=1
)

"""## Evaluasi Model"""

# Plot akurasi
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Akurasi Model')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend()
plt.grid(True)

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

train_loss, train_acc = model.evaluate(train_generator)
print(f"Akurasi di train set: {train_acc:.2%}")

val_loss, val_acc = model.evaluate(validation_generator)
print(f"Akurasi di val set: {val_acc:.2%}")

test_loss, test_acc = model.evaluate(test_generator)
print(f"Akurasi di test set: {test_acc:.2%}")

"""## Menyimpan Model"""

# Menyimpan model yang telah dilatih (termasuk arsitektur, bobot, dan konfigurasi training) ke dalam file bernama model.h5 dengan format HDF5
model.save("model.h5")

# Mengonversi model Keras (model.h5) ke format TensorFlow.js, sehingga bisa dijalankan di browser atau aplikasi JavaScript
!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

# Simpan model ke format SavedModel
save_path = "saved_model"
tf.saved_model.save(model, save_path)

print(f"Model berhasil disimpan dalam format SavedModel di folder: {save_path}")

print(train_generator.class_indices)

"""## Inference Model"""

# Upload file
uploaded = files.upload()

# Ambil nama file dari hasil upload
img_path = list(uploaded.keys())[0]
print("File uploaded:", img_path)

# Load gambar dengan path yang benar (file sudah ada di root folder)
img = image.load_img(img_path, target_size=(150, 150))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = img_array / 255.0  # jika model perlu normalisasi

# Load model SavedModel dengan TFSMLayer
from tensorflow import keras
saved_model_path = 'saved_model'
tfsmlayer = keras.layers.TFSMLayer(saved_model_path, call_endpoint='serving_default')
model = keras.Sequential([tfsmlayer])

# Prediksi
predictions = model.predict(img_array)
print(predictions)

def tampilkan_prediksi_daur_ulang(predictions, img):
    import numpy as np
    import matplotlib.pyplot as plt

    # Ambil output pertama dari predictions
    output_key = list(predictions.keys())[0]
    pred_array = predictions[output_key]

    # Label kelas dan remap ke kategori utama
    class_labels = ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']
    remap_dict = {
        'battery': 'residu',
        'biological': 'organik',
        'cardboard': 'anorganik',
        'clothes': 'residu',
        'glass': 'anorganik',
        'metal': 'anorganik',
        'paper': 'anorganik',
        'plastic': 'anorganik',
        'shoes': 'residu',
        'trash': 'residu'
    }

    # Info daur ulang spesifik berdasarkan label
    spesifik_info = {
        'battery': 'Baterai bekas mengandung bahan logam berat seperti merkuri, timbal, kadmium, dan litium. Harap kumpulkan, pisahkan, dan bawa ke dropbox limbah B3 atau e-waste center agar tidak mencemari lingkungan.',
        'biological': 'Sisa makanan dan bahan organik dapat dikomposkan untuk menjadi pupuk alami. Ini membantu dalam mengurangi intensitas sampah serta menghasilkan pupuk alami yang bermanfaat untuk tanaman.',
        'cardboard': 'Kardus yang sudah tidak digunakan dapat dilipat dan dijual ke pengepul atau diserahkan ke tempat daur ulang. Selain itu, kardus bekas dapat dibuat sebagai produk DIY (Do It Yourself), dapat mengurangi penumpukan sampah di tempat pembuangan.',
        'clothes': 'Pakaian lama yang masih layak digunakan dapat disumbangkan atau digunakan kembali. Apabila sudah rusak, sebaiknya dibuang ke dalam tempat sampah jenis residu supaya tidak mencemari sampah lainnya.',
        'glass': 'Harap pisahkan dan kumpulkan botol dan kaca dengan hati-hati. Pastikan tidak pecah dan serahkan pada tempat daur ulang kaca supaya dapat diproses kembali.',
        'metal': 'Kaleng dan jenis logam lainnya dapat dibersihkan dan dijual ke pengepul logam. Hal ini karena dapat membantu dalam mengurangi limbah sampah.',
        'paper': 'Kertas bersih dapat dikumpulkan dan dijual ke bank sampah atau tempat daur ulang. Pastikan pula kertas tidak tercampur dengan jenis sampah lain supaya hasil daur ulang maksimal',
        'plastic': 'Plastik harus dibersihkan dulu sebelum didaur ulang. Pisahkan plastik dengan jenis sampah residu supaya proses daur ulang dapat berjalan dengan baik.',
        'shoes': 'Sepatu lama yang masih layak dapat disumbangkan, sedangkan jika sudah rusak buang pada tempat sampah residu.',
        'trash': 'Jenis sampah ini tidak dapat didaur ulang. Buang sesuai dengan prosedur kebersihan dan jangan campur dengan sampah organik dan anorganik.'
    }

    # Prediksi kelas dan kategori utama
    predicted_class = np.argmax(pred_array, axis=1)
    predicted_label = class_labels[predicted_class[0]]
    main_category = remap_dict[predicted_label]
    probability = pred_array[0][predicted_class[0]]

    # Ambil info spesifik dan umum
    info_spesifik = spesifik_info.get(predicted_label, 'Tidak ada info spesifik.')

    # Tampilkan gambar dan info prediksi
    plt.imshow(img)
    plt.axis('off')
    plt.title(
        f"Prediksi: {predicted_label} ({main_category})\n"
        f"Probabilitas: {probability:.4f}\n\n"
        f"Cara Daur Ulang:\n{info_spesifik}\n\n"
    )
    plt.show()

# Contoh pemakaian:
tampilkan_prediksi_daur_ulang(predictions, img)